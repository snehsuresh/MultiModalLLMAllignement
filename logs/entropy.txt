 python main.py --use_pretrained --loss_strategy entropy
/scratch3/workspace/spillai_umassd_edu-pfolio/pfolio_env/lib/python3.12/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a "-quickgelu" suffix or enable with a flag.
  warnings.warn(
/scratch3/workspace/spillai_umassd_edu-pfolio/pfolio_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Training on device: cuda | Using Pretrained: True | Loss Strategy: entropy
Epoch [1/30] Batch [10/63] Loss: 6.7748
Epoch [1/30] Batch [20/63] Loss: 6.4541
Epoch [1/30] Batch [30/63] Loss: 5.9160
Epoch [1/30] Batch [40/63] Loss: 5.2559
Epoch [1/30] Batch [50/63] Loss: 5.1268
Epoch [1/30] Batch [60/63] Loss: 5.3637
--- Epoch [1/30] Average Loss: 5.9345 ---
Epoch [2/30] Batch [10/63] Loss: 4.9618
Epoch [2/30] Batch [20/63] Loss: 4.5420
Epoch [2/30] Batch [30/63] Loss: 4.2793
Epoch [2/30] Batch [40/63] Loss: 4.4696
Epoch [2/30] Batch [50/63] Loss: 4.2397
Epoch [2/30] Batch [60/63] Loss: 3.8351
--- Epoch [2/30] Average Loss: 4.3188 ---
Epoch [3/30] Batch [10/63] Loss: 3.5905
Epoch [3/30] Batch [20/63] Loss: 3.2305
Epoch [3/30] Batch [30/63] Loss: 3.5028
Epoch [3/30] Batch [40/63] Loss: 3.3594
Epoch [3/30] Batch [50/63] Loss: 3.3599
Epoch [3/30] Batch [60/63] Loss: 3.4319
--- Epoch [3/30] Average Loss: 3.4834 ---
Epoch [4/30] Batch [10/63] Loss: 3.3017
Epoch [4/30] Batch [20/63] Loss: 2.6001
Epoch [4/30] Batch [30/63] Loss: 3.0322
Epoch [4/30] Batch [40/63] Loss: 2.7738
Epoch [4/30] Batch [50/63] Loss: 2.6512
Epoch [4/30] Batch [60/63] Loss: 2.7849
--- Epoch [4/30] Average Loss: 2.9856 ---
Epoch [5/30] Batch [10/63] Loss: 2.7509
Epoch [5/30] Batch [20/63] Loss: 2.7715
Epoch [5/30] Batch [30/63] Loss: 2.3388
Epoch [5/30] Batch [40/63] Loss: 2.7709
Epoch [5/30] Batch [50/63] Loss: 2.4585
Epoch [5/30] Batch [60/63] Loss: 2.3429
--- Epoch [5/30] Average Loss: 2.5817 ---
Epoch [6/30] Batch [10/63] Loss: 2.6936
Epoch [6/30] Batch [20/63] Loss: 2.2282
Epoch [6/30] Batch [30/63] Loss: 2.1950
Epoch [6/30] Batch [40/63] Loss: 2.4953
Epoch [6/30] Batch [50/63] Loss: 2.1331
Epoch [6/30] Batch [60/63] Loss: 2.1903
--- Epoch [6/30] Average Loss: 2.3144 ---
Epoch [7/30] Batch [10/63] Loss: 2.1439
Epoch [7/30] Batch [20/63] Loss: 2.1577
Epoch [7/30] Batch [30/63] Loss: 1.8824
Epoch [7/30] Batch [40/63] Loss: 2.3131
Epoch [7/30] Batch [50/63] Loss: 2.1066
Epoch [7/30] Batch [60/63] Loss: 1.6538
--- Epoch [7/30] Average Loss: 2.1461 ---
Epoch [8/30] Batch [10/63] Loss: 1.5236
Epoch [8/30] Batch [20/63] Loss: 1.8372
Epoch [8/30] Batch [30/63] Loss: 1.8333
Epoch [8/30] Batch [40/63] Loss: 2.6151
Epoch [8/30] Batch [50/63] Loss: 2.4752
Epoch [8/30] Batch [60/63] Loss: 2.0951
--- Epoch [8/30] Average Loss: 1.9768 ---
Epoch [9/30] Batch [10/63] Loss: 1.9211
Epoch [9/30] Batch [20/63] Loss: 1.9353
Epoch [9/30] Batch [30/63] Loss: 1.3718
Epoch [9/30] Batch [40/63] Loss: 1.6956
Epoch [9/30] Batch [50/63] Loss: 1.5898
Epoch [9/30] Batch [60/63] Loss: 2.1965
--- Epoch [9/30] Average Loss: 1.7638 ---
Epoch [10/30] Batch [10/63] Loss: 1.8571
Epoch [10/30] Batch [20/63] Loss: 1.7507
Epoch [10/30] Batch [30/63] Loss: 1.5535
Epoch [10/30] Batch [40/63] Loss: 1.6393
Epoch [10/30] Batch [50/63] Loss: 1.8216
Epoch [10/30] Batch [60/63] Loss: 1.6144
--- Epoch [10/30] Average Loss: 1.7064 ---
Epoch [11/30] Batch [10/63] Loss: 1.4972
Epoch [11/30] Batch [20/63] Loss: 1.4672
Epoch [11/30] Batch [30/63] Loss: 1.5531
Epoch [11/30] Batch [40/63] Loss: 1.4162
Epoch [11/30] Batch [50/63] Loss: 1.4779
Epoch [11/30] Batch [60/63] Loss: 1.3922
--- Epoch [11/30] Average Loss: 1.5479 ---
Epoch [12/30] Batch [10/63] Loss: 1.5739
Epoch [12/30] Batch [20/63] Loss: 1.3150
Epoch [12/30] Batch [30/63] Loss: 1.4873
Epoch [12/30] Batch [40/63] Loss: 1.5255
Epoch [12/30] Batch [50/63] Loss: 1.8944
Epoch [12/30] Batch [60/63] Loss: 1.4561
--- Epoch [12/30] Average Loss: 1.4894 ---
Epoch [13/30] Batch [10/63] Loss: 1.7347
Epoch [13/30] Batch [20/63] Loss: 1.3669
Epoch [13/30] Batch [30/63] Loss: 1.4260
Epoch [13/30] Batch [40/63] Loss: 1.3251
Epoch [13/30] Batch [50/63] Loss: 1.2672
Epoch [13/30] Batch [60/63] Loss: 1.0347
--- Epoch [13/30] Average Loss: 1.4044 ---
Epoch [14/30] Batch [10/63] Loss: 1.7379
Epoch [14/30] Batch [20/63] Loss: 1.1082
Epoch [14/30] Batch [30/63] Loss: 1.2157
Epoch [14/30] Batch [40/63] Loss: 1.3731
Epoch [14/30] Batch [50/63] Loss: 1.3325
Epoch [14/30] Batch [60/63] Loss: 1.3977
--- Epoch [14/30] Average Loss: 1.3370 ---
Epoch [15/30] Batch [10/63] Loss: 1.5736
Epoch [15/30] Batch [20/63] Loss: 1.1032
Epoch [15/30] Batch [30/63] Loss: 1.4376
Epoch [15/30] Batch [40/63] Loss: 1.1272
Epoch [15/30] Batch [50/63] Loss: 1.1983
Epoch [15/30] Batch [60/63] Loss: 1.0095
--- Epoch [15/30] Average Loss: 1.2588 ---
Epoch [16/30] Batch [10/63] Loss: 1.4232
Epoch [16/30] Batch [20/63] Loss: 1.3454
Epoch [16/30] Batch [30/63] Loss: 1.2132
Epoch [16/30] Batch [40/63] Loss: 1.1810
Epoch [16/30] Batch [50/63] Loss: 1.4152
Epoch [16/30] Batch [60/63] Loss: 1.3757
--- Epoch [16/30] Average Loss: 1.2288 ---
Epoch [17/30] Batch [10/63] Loss: 1.0887
Epoch [17/30] Batch [20/63] Loss: 1.0571
Epoch [17/30] Batch [30/63] Loss: 1.1306
Epoch [17/30] Batch [40/63] Loss: 1.4820
Epoch [17/30] Batch [50/63] Loss: 1.3480
Epoch [17/30] Batch [60/63] Loss: 1.2044
--- Epoch [17/30] Average Loss: 1.1747 ---
Epoch [18/30] Batch [10/63] Loss: 1.0808
Epoch [18/30] Batch [20/63] Loss: 0.8499
Epoch [18/30] Batch [30/63] Loss: 1.0471
Epoch [18/30] Batch [40/63] Loss: 0.9790
Epoch [18/30] Batch [50/63] Loss: 0.6993
Epoch [18/30] Batch [60/63] Loss: 1.0592
--- Epoch [18/30] Average Loss: 1.0918 ---
Epoch [19/30] Batch [10/63] Loss: 1.1430
Epoch [19/30] Batch [20/63] Loss: 1.2909
Epoch [19/30] Batch [30/63] Loss: 1.0110
Epoch [19/30] Batch [40/63] Loss: 0.9631
Epoch [19/30] Batch [50/63] Loss: 1.1555
Epoch [19/30] Batch [60/63] Loss: 0.9662
--- Epoch [19/30] Average Loss: 1.0583 ---
Epoch [20/30] Batch [10/63] Loss: 0.9815
Epoch [20/30] Batch [20/63] Loss: 0.9356
Epoch [20/30] Batch [30/63] Loss: 0.8855
Epoch [20/30] Batch [40/63] Loss: 1.2531
Epoch [20/30] Batch [50/63] Loss: 0.8481
Epoch [20/30] Batch [60/63] Loss: 1.3842
--- Epoch [20/30] Average Loss: 1.0092 ---
Epoch [21/30] Batch [10/63] Loss: 0.6489
Epoch [21/30] Batch [20/63] Loss: 1.1273
Epoch [21/30] Batch [30/63] Loss: 0.8670
Epoch [21/30] Batch [40/63] Loss: 1.0530
Epoch [21/30] Batch [50/63] Loss: 0.8355
Epoch [21/30] Batch [60/63] Loss: 1.0144
--- Epoch [21/30] Average Loss: 0.9927 ---
Epoch [22/30] Batch [10/63] Loss: 1.0609
Epoch [22/30] Batch [20/63] Loss: 0.9295
Epoch [22/30] Batch [30/63] Loss: 1.0746
Epoch [22/30] Batch [40/63] Loss: 0.8606
Epoch [22/30] Batch [50/63] Loss: 0.7713
Epoch [22/30] Batch [60/63] Loss: 0.9208
--- Epoch [22/30] Average Loss: 0.9456 ---
Epoch [23/30] Batch [10/63] Loss: 0.6946
Epoch [23/30] Batch [20/63] Loss: 0.8131
Epoch [23/30] Batch [30/63] Loss: 1.0953
Epoch [23/30] Batch [40/63] Loss: 0.8630
Epoch [23/30] Batch [50/63] Loss: 1.0616
Epoch [23/30] Batch [60/63] Loss: 0.9143
--- Epoch [23/30] Average Loss: 0.9225 ---
Epoch [24/30] Batch [10/63] Loss: 1.0630
Epoch [24/30] Batch [20/63] Loss: 1.1926
Epoch [24/30] Batch [30/63] Loss: 0.8917
Epoch [24/30] Batch [40/63] Loss: 0.9159
Epoch [24/30] Batch [50/63] Loss: 0.9460
Epoch [24/30] Batch [60/63] Loss: 0.8731
--- Epoch [24/30] Average Loss: 0.8806 ---
Epoch [25/30] Batch [10/63] Loss: 1.0679
Epoch [25/30] Batch [20/63] Loss: 0.6837
Epoch [25/30] Batch [30/63] Loss: 1.0253
Epoch [25/30] Batch [40/63] Loss: 0.5580
Epoch [25/30] Batch [50/63] Loss: 1.0877
Epoch [25/30] Batch [60/63] Loss: 0.9747
--- Epoch [25/30] Average Loss: 0.8509 ---
Epoch [26/30] Batch [10/63] Loss: 0.8220
Epoch [26/30] Batch [20/63] Loss: 0.6536
Epoch [26/30] Batch [30/63] Loss: 0.7772
Epoch [26/30] Batch [40/63] Loss: 1.2467
Epoch [26/30] Batch [50/63] Loss: 0.7501
Epoch [26/30] Batch [60/63] Loss: 0.9194
--- Epoch [26/30] Average Loss: 0.8662 ---
Epoch [27/30] Batch [10/63] Loss: 0.9376
Epoch [27/30] Batch [20/63] Loss: 0.7061
Epoch [27/30] Batch [30/63] Loss: 0.5867
Epoch [27/30] Batch [40/63] Loss: 0.6797
Epoch [27/30] Batch [50/63] Loss: 0.7948
Epoch [27/30] Batch [60/63] Loss: 1.0118
--- Epoch [27/30] Average Loss: 0.8209 ---
Epoch [28/30] Batch [10/63] Loss: 0.5429
Epoch [28/30] Batch [20/63] Loss: 0.9464
Epoch [28/30] Batch [30/63] Loss: 0.6510
Epoch [28/30] Batch [40/63] Loss: 0.5968
Epoch [28/30] Batch [50/63] Loss: 0.9937
Epoch [28/30] Batch [60/63] Loss: 0.8084
--- Epoch [28/30] Average Loss: 0.7786 ---
Epoch [29/30] Batch [10/63] Loss: 0.9193
Epoch [29/30] Batch [20/63] Loss: 0.6536
Epoch [29/30] Batch [30/63] Loss: 1.1335
Epoch [29/30] Batch [40/63] Loss: 0.7092
Epoch [29/30] Batch [50/63] Loss: 0.7956
Epoch [29/30] Batch [60/63] Loss: 0.6800
--- Epoch [29/30] Average Loss: 0.7484 ---
Epoch [30/30] Batch [10/63] Loss: 0.9333
Epoch [30/30] Batch [20/63] Loss: 0.8055
Epoch [30/30] Batch [30/63] Loss: 0.5850
Epoch [30/30] Batch [40/63] Loss: 0.6668
Epoch [30/30] Batch [50/63] Loss: 0.8441
Epoch [30/30] Batch [60/63] Loss: 0.8048
--- Epoch [30/30] Average Loss: 0.7625 ---

Verifying Alignment:

Sample Cosine Similarity (Text-Image) for first 10 samples:
[0.52621686 0.70458794 0.57202625 0.6142944  0.6020286  0.5695237
 0.52667457 0.4296887  0.6364534  0.50274396]

Cosine Similarity Matrix (first 5 samples):
[[ 0.5262168   0.09495329 -0.16250806 -0.22250251  0.10664831]
 [ 0.05636011  0.70458794  0.06282282 -0.00175961 -0.32462403]
 [-0.18737714  0.17632823  0.57202625  0.5780091  -0.34390995]
 [-0.22484611  0.0273332   0.5248962   0.6142944  -0.22172613]
 [ 0.27921993 -0.08671241 -0.14778304 -0.3867028   0.6020286 ]]
Retrieval Metrics: {'R@1': 0.90625, 'R@5': 1.0, 'R@10': 1.0}

Running Stress Test (with injected noise):
Stress Test Results:
Clean Loss: 0.5805
Noisy Loss: 0.7195
Model saved as multimodal_model.pth