python main.py --use_pretrained --loss_strategy cosine_spread
/scratch3/workspace/spillai_umassd_edu-pfolio/pfolio_env/lib/python3.12/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a "-quickgelu" suffix or enable with a flag.
  warnings.warn(
/scratch3/workspace/spillai_umassd_edu-pfolio/pfolio_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Training on device: cuda | Using Pretrained: True | Loss Strategy: cosine_spread
Epoch [1/30] Batch [10/63] Loss: 6.5871
Epoch [1/30] Batch [20/63] Loss: 6.3023
Epoch [1/30] Batch [30/63] Loss: 5.7637
Epoch [1/30] Batch [40/63] Loss: 5.6253
Epoch [1/30] Batch [50/63] Loss: 5.2456
Epoch [1/30] Batch [60/63] Loss: 5.3762
--- Epoch [1/30] Average Loss: 5.9864 ---
Epoch [2/30] Batch [10/63] Loss: 4.6700
Epoch [2/30] Batch [20/63] Loss: 4.5859
Epoch [2/30] Batch [30/63] Loss: 4.4420
Epoch [2/30] Batch [40/63] Loss: 4.4362
Epoch [2/30] Batch [50/63] Loss: 3.9707
Epoch [2/30] Batch [60/63] Loss: 4.0910
--- Epoch [2/30] Average Loss: 4.3696 ---
Epoch [3/30] Batch [10/63] Loss: 3.8522
Epoch [3/30] Batch [20/63] Loss: 4.0106
Epoch [3/30] Batch [30/63] Loss: 3.8743
Epoch [3/30] Batch [40/63] Loss: 3.6761
Epoch [3/30] Batch [50/63] Loss: 3.5491
Epoch [3/30] Batch [60/63] Loss: 3.2815
--- Epoch [3/30] Average Loss: 3.5654 ---
Epoch [4/30] Batch [10/63] Loss: 3.0355
Epoch [4/30] Batch [20/63] Loss: 2.9322
Epoch [4/30] Batch [30/63] Loss: 3.4389
Epoch [4/30] Batch [40/63] Loss: 3.4392
Epoch [4/30] Batch [50/63] Loss: 2.3573
Epoch [4/30] Batch [60/63] Loss: 3.1778
--- Epoch [4/30] Average Loss: 3.0401 ---
Epoch [5/30] Batch [10/63] Loss: 2.5116
Epoch [5/30] Batch [20/63] Loss: 2.8323
Epoch [5/30] Batch [30/63] Loss: 2.6904
Epoch [5/30] Batch [40/63] Loss: 2.3169
Epoch [5/30] Batch [50/63] Loss: 2.7971
Epoch [5/30] Batch [60/63] Loss: 2.6717
--- Epoch [5/30] Average Loss: 2.6742 ---
Epoch [6/30] Batch [10/63] Loss: 2.6772
Epoch [6/30] Batch [20/63] Loss: 2.3571
Epoch [6/30] Batch [30/63] Loss: 2.5178
Epoch [6/30] Batch [40/63] Loss: 2.5507
Epoch [6/30] Batch [50/63] Loss: 2.3573
Epoch [6/30] Batch [60/63] Loss: 2.5476
--- Epoch [6/30] Average Loss: 2.4031 ---
Epoch [7/30] Batch [10/63] Loss: 2.1113
Epoch [7/30] Batch [20/63] Loss: 1.9001
Epoch [7/30] Batch [30/63] Loss: 2.6531
Epoch [7/30] Batch [40/63] Loss: 2.2163
Epoch [7/30] Batch [50/63] Loss: 2.1414
Epoch [7/30] Batch [60/63] Loss: 2.0129
--- Epoch [7/30] Average Loss: 2.1816 ---
Epoch [8/30] Batch [10/63] Loss: 2.0585
Epoch [8/30] Batch [20/63] Loss: 1.9387
Epoch [8/30] Batch [30/63] Loss: 1.9786
Epoch [8/30] Batch [40/63] Loss: 1.8707
Epoch [8/30] Batch [50/63] Loss: 2.0294
Epoch [8/30] Batch [60/63] Loss: 1.9418
--- Epoch [8/30] Average Loss: 1.9997 ---
Epoch [9/30] Batch [10/63] Loss: 2.0492
Epoch [9/30] Batch [20/63] Loss: 1.6338
Epoch [9/30] Batch [30/63] Loss: 2.0241
Epoch [9/30] Batch [40/63] Loss: 1.8439
Epoch [9/30] Batch [50/63] Loss: 1.8719
Epoch [9/30] Batch [60/63] Loss: 1.9847
--- Epoch [9/30] Average Loss: 1.8466 ---
Epoch [10/30] Batch [10/63] Loss: 1.5606
Epoch [10/30] Batch [20/63] Loss: 1.8935
Epoch [10/30] Batch [30/63] Loss: 1.7719
Epoch [10/30] Batch [40/63] Loss: 1.6032
Epoch [10/30] Batch [50/63] Loss: 1.6298
Epoch [10/30] Batch [60/63] Loss: 1.4978
--- Epoch [10/30] Average Loss: 1.6886 ---
Epoch [11/30] Batch [10/63] Loss: 1.7691
Epoch [11/30] Batch [20/63] Loss: 1.4007
Epoch [11/30] Batch [30/63] Loss: 1.8028
Epoch [11/30] Batch [40/63] Loss: 2.0887
Epoch [11/30] Batch [50/63] Loss: 1.3134
Epoch [11/30] Batch [60/63] Loss: 1.5885
--- Epoch [11/30] Average Loss: 1.5702 ---
Epoch [12/30] Batch [10/63] Loss: 1.9468
Epoch [12/30] Batch [20/63] Loss: 1.4443
Epoch [12/30] Batch [30/63] Loss: 1.5371
Epoch [12/30] Batch [40/63] Loss: 1.5981
Epoch [12/30] Batch [50/63] Loss: 1.3399
Epoch [12/30] Batch [60/63] Loss: 1.4763
--- Epoch [12/30] Average Loss: 1.5153 ---
Epoch [13/30] Batch [10/63] Loss: 1.5932
Epoch [13/30] Batch [20/63] Loss: 1.2220
Epoch [13/30] Batch [30/63] Loss: 1.3829
Epoch [13/30] Batch [40/63] Loss: 1.8998
Epoch [13/30] Batch [50/63] Loss: 1.4695
Epoch [13/30] Batch [60/63] Loss: 1.5061
--- Epoch [13/30] Average Loss: 1.4272 ---
Epoch [14/30] Batch [10/63] Loss: 1.2533
Epoch [14/30] Batch [20/63] Loss: 1.2802
Epoch [14/30] Batch [30/63] Loss: 1.1680
Epoch [14/30] Batch [40/63] Loss: 1.4280
Epoch [14/30] Batch [50/63] Loss: 1.5104
Epoch [14/30] Batch [60/63] Loss: 1.7712
--- Epoch [14/30] Average Loss: 1.3612 ---
Epoch [15/30] Batch [10/63] Loss: 1.4312
Epoch [15/30] Batch [20/63] Loss: 1.0775
Epoch [15/30] Batch [30/63] Loss: 0.8841
Epoch [15/30] Batch [40/63] Loss: 1.4821
Epoch [15/30] Batch [50/63] Loss: 1.4066
Epoch [15/30] Batch [60/63] Loss: 1.5742
--- Epoch [15/30] Average Loss: 1.2894 ---
Epoch [16/30] Batch [10/63] Loss: 1.2339
Epoch [16/30] Batch [20/63] Loss: 1.3296
Epoch [16/30] Batch [30/63] Loss: 0.9238
Epoch [16/30] Batch [40/63] Loss: 1.2003
Epoch [16/30] Batch [50/63] Loss: 1.5088
Epoch [16/30] Batch [60/63] Loss: 1.3113
--- Epoch [16/30] Average Loss: 1.2267 ---
Epoch [17/30] Batch [10/63] Loss: 1.1943
Epoch [17/30] Batch [20/63] Loss: 1.1296
Epoch [17/30] Batch [30/63] Loss: 1.0964
Epoch [17/30] Batch [40/63] Loss: 1.1103
Epoch [17/30] Batch [50/63] Loss: 1.4383
Epoch [17/30] Batch [60/63] Loss: 1.1062
--- Epoch [17/30] Average Loss: 1.1642 ---
Epoch [18/30] Batch [10/63] Loss: 1.3498
Epoch [18/30] Batch [20/63] Loss: 1.1925
Epoch [18/30] Batch [30/63] Loss: 1.1242
Epoch [18/30] Batch [40/63] Loss: 1.2991
Epoch [18/30] Batch [50/63] Loss: 0.7837
Epoch [18/30] Batch [60/63] Loss: 1.4517
--- Epoch [18/30] Average Loss: 1.1280 ---
Epoch [19/30] Batch [10/63] Loss: 1.1372
Epoch [19/30] Batch [20/63] Loss: 0.9314
Epoch [19/30] Batch [30/63] Loss: 0.8290
Epoch [19/30] Batch [40/63] Loss: 1.0319
Epoch [19/30] Batch [50/63] Loss: 1.0945
Epoch [19/30] Batch [60/63] Loss: 1.2245
--- Epoch [19/30] Average Loss: 1.0679 ---
Epoch [20/30] Batch [10/63] Loss: 1.0340
Epoch [20/30] Batch [20/63] Loss: 1.2748
Epoch [20/30] Batch [30/63] Loss: 1.1497
Epoch [20/30] Batch [40/63] Loss: 1.2929
Epoch [20/30] Batch [50/63] Loss: 1.1848
Epoch [20/30] Batch [60/63] Loss: 1.2306
--- Epoch [20/30] Average Loss: 1.0667 ---
Epoch [21/30] Batch [10/63] Loss: 0.8600
Epoch [21/30] Batch [20/63] Loss: 0.9770
Epoch [21/30] Batch [30/63] Loss: 0.8799
Epoch [21/30] Batch [40/63] Loss: 0.9539
Epoch [21/30] Batch [50/63] Loss: 1.0554
Epoch [21/30] Batch [60/63] Loss: 1.1732
--- Epoch [21/30] Average Loss: 1.0107 ---
Epoch [22/30] Batch [10/63] Loss: 0.7408
Epoch [22/30] Batch [20/63] Loss: 0.6517
Epoch [22/30] Batch [30/63] Loss: 0.8106
Epoch [22/30] Batch [40/63] Loss: 0.9349
Epoch [22/30] Batch [50/63] Loss: 0.9869
Epoch [22/30] Batch [60/63] Loss: 1.1336
--- Epoch [22/30] Average Loss: 0.9463 ---
Epoch [23/30] Batch [10/63] Loss: 1.0871
Epoch [23/30] Batch [20/63] Loss: 0.8788
Epoch [23/30] Batch [30/63] Loss: 0.9996
Epoch [23/30] Batch [40/63] Loss: 0.8181
Epoch [23/30] Batch [50/63] Loss: 0.8333
Epoch [23/30] Batch [60/63] Loss: 0.7410
--- Epoch [23/30] Average Loss: 0.8938 ---
Epoch [24/30] Batch [10/63] Loss: 1.2408
Epoch [24/30] Batch [20/63] Loss: 1.0116
Epoch [24/30] Batch [30/63] Loss: 0.9299
Epoch [24/30] Batch [40/63] Loss: 0.6628
Epoch [24/30] Batch [50/63] Loss: 1.0962
Epoch [24/30] Batch [60/63] Loss: 1.0430
--- Epoch [24/30] Average Loss: 0.8829 ---
Epoch [25/30] Batch [10/63] Loss: 0.5046
Epoch [25/30] Batch [20/63] Loss: 0.6764
Epoch [25/30] Batch [30/63] Loss: 0.7301
Epoch [25/30] Batch [40/63] Loss: 1.1403
Epoch [25/30] Batch [50/63] Loss: 0.8212
Epoch [25/30] Batch [60/63] Loss: 0.6723
--- Epoch [25/30] Average Loss: 0.8882 ---
Epoch [26/30] Batch [10/63] Loss: 1.5758
Epoch [26/30] Batch [20/63] Loss: 0.6905
Epoch [26/30] Batch [30/63] Loss: 0.6967
Epoch [26/30] Batch [40/63] Loss: 0.7923
Epoch [26/30] Batch [50/63] Loss: 0.8657
Epoch [26/30] Batch [60/63] Loss: 0.7858
--- Epoch [26/30] Average Loss: 0.8653 ---
Epoch [27/30] Batch [10/63] Loss: 0.6111
Epoch [27/30] Batch [20/63] Loss: 0.6885
Epoch [27/30] Batch [30/63] Loss: 0.8180
Epoch [27/30] Batch [40/63] Loss: 0.5023
Epoch [27/30] Batch [50/63] Loss: 0.8527
Epoch [27/30] Batch [60/63] Loss: 0.9606
--- Epoch [27/30] Average Loss: 0.8092 ---
Epoch [28/30] Batch [10/63] Loss: 0.9599
Epoch [28/30] Batch [20/63] Loss: 0.7041
Epoch [28/30] Batch [30/63] Loss: 0.9608
Epoch [28/30] Batch [40/63] Loss: 0.8097
Epoch [28/30] Batch [50/63] Loss: 0.8805
Epoch [28/30] Batch [60/63] Loss: 0.8215
--- Epoch [28/30] Average Loss: 0.7929 ---
Epoch [29/30] Batch [10/63] Loss: 0.9107
Epoch [29/30] Batch [20/63] Loss: 0.8977
Epoch [29/30] Batch [30/63] Loss: 0.5373
Epoch [29/30] Batch [40/63] Loss: 0.6488
Epoch [29/30] Batch [50/63] Loss: 0.7735
Epoch [29/30] Batch [60/63] Loss: 0.7339
--- Epoch [29/30] Average Loss: 0.7756 ---
Epoch [30/30] Batch [10/63] Loss: 0.6770
Epoch [30/30] Batch [20/63] Loss: 0.7321
Epoch [30/30] Batch [30/63] Loss: 0.5860
Epoch [30/30] Batch [40/63] Loss: 0.8244
Epoch [30/30] Batch [50/63] Loss: 0.8114
Epoch [30/30] Batch [60/63] Loss: 0.9689
--- Epoch [30/30] Average Loss: 0.7852 ---

Verifying Alignment:

Sample Cosine Similarity (Text-Image) for first 10 samples:
[0.4356795  0.6491472  0.60962    0.5632967  0.5757004  0.5451433
 0.5441791  0.60925007 0.6034742  0.68564254]

Cosine Similarity Matrix (first 5 samples):
[[ 0.4356795  -0.07796581  0.04914679 -0.0997546   0.04282746]
 [-0.01938301  0.6491472   0.10500989 -0.11733979 -0.20739496]
 [ 0.16934827  0.1494751   0.60962     0.10206122 -0.17695603]
 [ 0.03655649  0.03406239  0.11884972  0.5632967   0.01038268]
 [-0.13942963  0.1559042  -0.1081212  -0.09517381  0.57570046]]
Retrieval Metrics: {'R@1': 0.90625, 'R@5': 1.0, 'R@10': 1.0}

Running Stress Test (with injected noise):
Stress Test Results:
Clean Loss: 0.4612
Noisy Loss: 0.6290
Model saved as multimodal_model.pth